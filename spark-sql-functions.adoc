== [[functions]] Standard Functions -- functions Object

`org.apache.spark.sql.functions` object defines built-in <<standard-functions, standard functions>> to work with (values produced by) <<spark-sql-Column.adoc#, columns>>.

You can access the standard functions using the following `import` statement in your Scala application:

[source, scala]
----
import org.apache.spark.sql.functions._
----

There are over 200 functions in the `functions` object.

```
scala> spark.catalog.listFunctions.count
res1: Long = 251
```

[[standard-functions]]
.(Subset of) Standard Functions in Spark SQL
[align="center",cols="1,1,2",width="100%",options="header"]
|===
|
|Name
|Description

.4+^.^| [[aggregate-functions]][[agg_funcs]] *Aggregate functions*

| <<count, count>>
|

| <<first, first>>
a|

[source, scala]
----
first(e: Column): Column
first(e: Column, ignoreNulls: Boolean): Column
first(columnName: String): Column
first(columnName: String, ignoreNulls: Boolean): Column
----

Returns the first value in a group. Returns the first non-null value when `ignoreNulls` flag on. If all values are null, then returns null.

| <<grouping, grouping>>
| Indicates whether a specified column is aggregated or not

| <<grouping_id, grouping_id>>
| Computes the level of grouping

.8+^.^| *Collection functions*

| [[array_contains]] <<spark-sql-functions-collection.adoc#array_contains, array_contains>>
a|

[source, scala]
----
array_contains(column: Column, value: Any): Column
----

| [[explode]] link:spark-sql-functions-collection.adoc#explode[explode]
a|

[source, scala]
----
explode(e: Column): Column
----

| [[explode_outer]] link:spark-sql-functions-collection.adoc#explode_outer[explode_outer]
a|

[source, scala]
----
explode_outer(e: Column): Column
----

Creates a new row for each element in the given array or map column. If the array/map is `null` or empty then `null` is produced.

| [[from_json]] link:spark-sql-functions-collection.adoc#from_json[from_json]
a|

[source, scala]
----
from_json(e: Column, schema: DataType): Column
from_json(e: Column, schema: DataType, options: Map[String, String]): Column
from_json(e: Column, schema: String, options: Map[String, String]): Column // <1>
from_json(e: Column, schema: StructType): Column
from_json(e: Column, schema: StructType, options: Map[String, String]): Column
----
<1> *New in 2.3.0*

Parses a column with a JSON string into a link:spark-sql-StructType.adoc[StructType] or link:spark-sql-DataType.adoc#ArrayType[ArrayType] of `StructType` elements with the specified schema.

| [[map_keys]] <<spark-sql-functions-collection.adoc#map_keys, map_keys>>
a|

[source, scala]
----
map_keys(e: Column): Column
----

*(new in 2.3.0)*

| [[map_values]] <<spark-sql-functions-collection.adoc#map_values, map_values>>
a|

[source, scala]
----
map_values(e: Column): Column
----

*(new in 2.3.0)*

| [[posexplode]] <<spark-sql-functions-collection.adoc#posexplode, posexplode>>
a|

[source, scala]
----
posexplode(e: Column): Column
----

| [[posexplode_outer]] <<spark-sql-functions-collection.adoc#posexplode_outer, posexplode_outer>>
a|

[source, scala]
----
posexplode_outer(e: Column): Column
----

.5+^.^| *Date and time functions*
| link:spark-sql-functions-datetime.adoc#current_timestamp[current_timestamp]
|

| link:spark-sql-functions-datetime.adoc#to_date[to_date]
|

| link:spark-sql-functions-datetime.adoc#to_timestamp[to_timestamp]
|

| link:spark-sql-functions-datetime.adoc#unix_timestamp[unix_timestamp]
| Converts current or specified time to Unix timestamp (in seconds)

| link:spark-sql-functions-datetime.adoc#window[window]
| Generates tumbling time windows

1+^.^| *Math functions*
| <<bin, bin>>
| Converts the value of a long column to binary format

.11+^.^| *Regular functions* (Non-aggregate functions)

| [[array]] link:spark-sql-functions-regular-functions.adoc#array[array]
|

| [[broadcast]] link:spark-sql-functions-regular-functions.adoc#broadcast[broadcast]
|

| [[coalesce]] link:spark-sql-functions-regular-functions.adoc#coalesce[coalesce]
| Gives the first non-``null`` value among the given columns or `null`

| [[col]][[column]] link:spark-sql-functions-regular-functions.adoc#col[col] and link:spark-sql-functions-regular-functions.adoc#column[column]
| Creating link:spark-sql-Column.adoc[Columns]

| [[expr]] link:spark-sql-functions-regular-functions.adoc#expr[expr]
|

| [[lit]] link:spark-sql-functions-regular-functions.adoc#lit[lit]
|

| [[map]] link:spark-sql-functions-regular-functions.adoc#map[map]
|

| <<spark-sql-functions-regular-functions.adoc#monotonically_increasing_id, monotonically_increasing_id>>
| [[monotonically_increasing_id]] Returns monotonically increasing 64-bit integers that are guaranteed to be monotonically increasing and unique, but not consecutive.

| [[struct]] link:spark-sql-functions-regular-functions.adoc#struct[struct]
|

| [[typedLit]] link:spark-sql-functions-regular-functions.adoc#typedLit[typedLit]
|

| [[when]] link:spark-sql-functions-regular-functions.adoc#when[when]
|

.2+^.^| *String functions*
| <<split, split>>
|

| <<upper, upper>>
|

1.2+^.^| *UDF functions*
| <<udf, udf>>
| Creating UDFs

| <<callUDF, callUDF>>
| Executing an UDF by name with variable-length list of columns

.11+^.^| [[window-functions]] *Window functions*

| [[cume_dist]] <<spark-sql-functions-windows.adoc#cume_dist, cume_dist>>
a|

[source, scala]
----
cume_dist(): Column
----

Computes the cumulative distribution of records across window partitions

| [[currentRow]] <<spark-sql-functions-windows.adoc#currentRow, currentRow>>
a|

[source, scala]
----
currentRow(): Column
----

*(new in 2.3.0)*

| [[dense_rank]] <<spark-sql-functions-windows.adoc#dense_rank, dense_rank>>
a|

[source, scala]
----
dense_rank(): Column
----

Computes the rank of records per window partition

| [[lag]] <<spark-sql-functions-windows.adoc#lag, lag>>
a|

[source, scala]
----
lag(e: Column, offset: Int): Column
lag(columnName: String, offset: Int): Column
lag(columnName: String, offset: Int, defaultValue: Any): Column
----

| [[lead]] <<spark-sql-functions-windows.adoc#lead, lead>>
a|

[source, scala]
----
lead(columnName: String, offset: Int): Column
lead(e: Column, offset: Int): Column
lead(columnName: String, offset: Int, defaultValue: Any): Column
lead(e: Column, offset: Int, defaultValue: Any): Column
----

| [[ntile]] <<spark-sql-functions-windows.adoc#ntile, ntile>>
a|

[source, scala]
----
ntile(n: Int): Column
----

Computes the ntile group

| [[percent_rank]] <<spark-sql-functions-windows.adoc#percent_rank, percent_rank>>
a|

[source, scala]
----
percent_rank(): Column
----

Computes the rank of records per window partition

| [[rank]] <<spark-sql-functions-windows.adoc#rank, rank>>
a|

[source, scala]
----
rank(): Column
----

Computes the rank of records per window partition

| [[row_number]] <<spark-sql-functions-windows.adoc#row_number, row_number>>
a|

[source, scala]
----
row_number(): Column
----

Computes the sequential numbering per window partition

| [[unboundedFollowing]] <<spark-sql-functions-windows.adoc#unboundedFollowing, unboundedFollowing>>
a|

[source, scala]
----
unboundedFollowing(): Column
----

*(new in 2.3.0)*

| [[unboundedPreceding]] <<spark-sql-functions-windows.adoc#unboundedPreceding, unboundedPreceding>>
a|

[source, scala]
----
unboundedPreceding(): Column
----

*(new in 2.3.0)*
|===

TIP: The page gives only a brief ovierview of the many functions available in `functions` object and so you should read the http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$[official documentation of the `functions` object].

=== [[count]] `count` Function

CAUTION: FIXME

=== [[callUDF]] Executing UDF by Name and Variable-Length Column List -- `callUDF` Function

[source, scala]
----
callUDF(udfName: String, cols: Column*): Column
----

`callUDF` executes an UDF by `udfName` and variable-length list of columns.

=== [[udf]] Defining UDFs -- `udf` Function

[source, scala]
----
udf(f: FunctionN[...]): UserDefinedFunction
----

The `udf` family of functions allows you to create link:spark-sql-udfs.adoc[user-defined functions (UDFs)] based on a user-defined function in Scala. It accepts `f` function of 0 to 10 arguments and the input and output types are automatically inferred (given the types of the respective input and output types of the function `f`).

[source, scala]
----
import org.apache.spark.sql.functions._
val _length: String => Int = _.length
val _lengthUDF = udf(_length)

// define a dataframe
val df = sc.parallelize(0 to 3).toDF("num")

// apply the user-defined function to "num" column
scala> df.withColumn("len", _lengthUDF($"num")).show
+---+---+
|num|len|
+---+---+
|  0|  1|
|  1|  1|
|  2|  1|
|  3|  1|
+---+---+
----

Since Spark 2.0.0, there is another variant of `udf` function:

[source, scala]
----
udf(f: AnyRef, dataType: DataType): UserDefinedFunction
----

`udf(f: AnyRef, dataType: DataType)` allows you to use a Scala closure for the function argument (as `f`) and explicitly declaring the output data type (as `dataType`).

[source, scala]
----
// given the dataframe above

import org.apache.spark.sql.types.IntegerType
val byTwo = udf((n: Int) => n * 2, IntegerType)

scala> df.withColumn("len", byTwo($"num")).show
+---+---+
|num|len|
+---+---+
|  0|  0|
|  1|  2|
|  2|  4|
|  3|  6|
+---+---+
----

=== [[split]] `split` Function

[source, scala]
----
split(str: Column, pattern: String): Column
----

`split` function splits `str` column using `pattern`. It returns a new `Column`.

NOTE: `split` UDF uses https://docs.oracle.com/javase/8/docs/api/java/lang/String.html#split-java.lang.String-int-[java.lang.String.split(String regex, int limit)] method.

[source, scala]
----
val df = Seq((0, "hello|world"), (1, "witaj|swiecie")).toDF("num", "input")
val withSplit = df.withColumn("split", split($"input", "[|]"))

scala> withSplit.show
+---+-------------+----------------+
|num|        input|           split|
+---+-------------+----------------+
|  0|  hello|world|  [hello, world]|
|  1|witaj|swiecie|[witaj, swiecie]|
+---+-------------+----------------+
----

NOTE: `.$|()[{^?*+\` are RegEx's meta characters and are considered special.

=== [[upper]] `upper` Function

[source, scala]
----
upper(e: Column): Column
----

`upper` function converts a string column into one with all letter upper. It returns a new `Column`.

NOTE: The following example uses two functions that accept a `Column` and return another to showcase how to chain them.

[source, scala]
----
val df = Seq((0,1,"hello"), (2,3,"world"), (2,4, "ala")).toDF("id", "val", "name")
val withUpperReversed = df.withColumn("upper", reverse(upper($"name")))

scala> withUpperReversed.show
+---+---+-----+-----+
| id|val| name|upper|
+---+---+-----+-----+
|  0|  1|hello|OLLEH|
|  2|  3|world|DLROW|
|  2|  4|  ala|  ALA|
+---+---+-----+-----+
----

=== [[grouping]] `grouping` Aggregate Function

[source, scala]
----
grouping(e: Column): Column
grouping(columnName: String): Column  // <1>
----
<1> Calls the first `grouping` with `columnName` as a `Column`

`grouping` is an aggregate function that indicates whether a specified column is aggregated or not and:

* returns `1` if the column is in a subtotal and is `NULL`
* returns `0` if the underlying value is `NULL` or any other value

NOTE: `grouping` can only be used with link:spark-sql-multi-dimensional-aggregation.adoc#cube[cube], link:spark-sql-multi-dimensional-aggregation.adoc#rollup[rollup] or `GROUPING SETS` multi-dimensional aggregate operators (and is verified when link:spark-sql-Analyzer-CheckAnalysis.adoc#Grouping[`Analyzer` does check analysis]).

From https://cwiki.apache.org/confluence/display/Hive/Enhanced&#43;Aggregation%2C&#43;Cube%2C&#43;Grouping&#43;and&#43;Rollup#EnhancedAggregation,Cube,GroupingandRollup-Grouping\_\_IDfunction[Hive's documentation about Grouping__ID function] (that can somehow help to understand `grouping`):

> When aggregates are displayed for a column its value is `null`. This may conflict in case the column itself has some `null` values. There needs to be some way to identify `NULL` in column, which means aggregate and `NULL` in column, which means value. `GROUPING__ID` function is the solution to that.

[source, scala]
----
val tmpWorkshops = Seq(
  ("Warsaw", 2016, 2),
  ("Toronto", 2016, 4),
  ("Toronto", 2017, 1)).toDF("city", "year", "count")

// there seems to be a bug with nulls
// and so the need for the following union
val cityNull = Seq(
  (null.asInstanceOf[String], 2016, 2)).toDF("city", "year", "count")

val workshops = tmpWorkshops union cityNull

scala> workshops.show
+-------+----+-----+
|   city|year|count|
+-------+----+-----+
| Warsaw|2016|    2|
|Toronto|2016|    4|
|Toronto|2017|    1|
|   null|2016|    2|
+-------+----+-----+

val q = workshops
  .cube("city", "year")
  .agg(grouping("city"), grouping("year")) // <-- grouping here
  .sort($"city".desc_nulls_last, $"year".desc_nulls_last)

scala> q.show
+-------+----+--------------+--------------+
|   city|year|grouping(city)|grouping(year)|
+-------+----+--------------+--------------+
| Warsaw|2016|             0|             0|
| Warsaw|null|             0|             1|
|Toronto|2017|             0|             0|
|Toronto|2016|             0|             0|
|Toronto|null|             0|             1|
|   null|2017|             1|             0|
|   null|2016|             1|             0|
|   null|2016|             0|             0|  <-- null is city
|   null|null|             0|             1|  <-- null is city
|   null|null|             1|             1|
+-------+----+--------------+--------------+
----

Internally, `grouping` creates a link:spark-sql-Column.adoc[Column] with `Grouping` expression.

```
val q = workshops.cube("city", "year").agg(grouping("city"))
scala> println(q.queryExecution.logical)
'Aggregate [cube(city#182, year#183)], [city#182, year#183, grouping('city) AS grouping(city)#705]
+- Union
   :- Project [_1#178 AS city#182, _2#179 AS year#183, _3#180 AS count#184]
   :  +- LocalRelation [_1#178, _2#179, _3#180]
   +- Project [_1#192 AS city#196, _2#193 AS year#197, _3#194 AS count#198]
      +- LocalRelation [_1#192, _2#193, _3#194]

scala> println(q.queryExecution.analyzed)
Aggregate [city#724, year#725, spark_grouping_id#721], [city#724, year#725, cast((shiftright(spark_grouping_id#721, 1) & 1) as tinyint) AS grouping(city)#720]
+- Expand [List(city#182, year#183, count#184, city#722, year#723, 0), List(city#182, year#183, count#184, city#722, null, 1), List(city#182, year#183, count#184, null, year#723, 2), List(city#182, year#183, count#184, null, null, 3)], [city#182, year#183, count#184, city#724, year#725, spark_grouping_id#721]
   +- Project [city#182, year#183, count#184, city#182 AS city#722, year#183 AS year#723]
      +- Union
         :- Project [_1#178 AS city#182, _2#179 AS year#183, _3#180 AS count#184]
         :  +- LocalRelation [_1#178, _2#179, _3#180]
         +- Project [_1#192 AS city#196, _2#193 AS year#197, _3#194 AS count#198]
            +- LocalRelation [_1#192, _2#193, _3#194]
```

NOTE: `grouping` was added to Spark SQL in https://issues.apache.org/jira/browse/SPARK-12706[[SPARK-12706\] support grouping/grouping_id function together group set].

=== [[grouping_id]] `grouping_id` Aggregate Function

[source, scala]
----
grouping_id(cols: Column*): Column
grouping_id(colName: String, colNames: String*): Column // <1>
----
<1> Calls the first `grouping_id` with `colName` and `colNames` as objects of type `Column`

`grouping_id` is an aggregate function that computes the level of grouping:

* `0` for combinations of each column
* `1` for subtotals of column 1
* `2` for subtotals of column 2
* And so on&hellip;

[source, scala]
----
val tmpWorkshops = Seq(
  ("Warsaw", 2016, 2),
  ("Toronto", 2016, 4),
  ("Toronto", 2017, 1)).toDF("city", "year", "count")

// there seems to be a bug with nulls
// and so the need for the following union
val cityNull = Seq(
  (null.asInstanceOf[String], 2016, 2)).toDF("city", "year", "count")

val workshops = tmpWorkshops union cityNull

scala> workshops.show
+-------+----+-----+
|   city|year|count|
+-------+----+-----+
| Warsaw|2016|    2|
|Toronto|2016|    4|
|Toronto|2017|    1|
|   null|2016|    2|
+-------+----+-----+

val query = workshops
  .cube("city", "year")
  .agg(grouping_id()) // <-- all grouping columns used
  .sort($"city".desc_nulls_last, $"year".desc_nulls_last)
scala> query.show
+-------+----+-------------+
|   city|year|grouping_id()|
+-------+----+-------------+
| Warsaw|2016|            0|
| Warsaw|null|            1|
|Toronto|2017|            0|
|Toronto|2016|            0|
|Toronto|null|            1|
|   null|2017|            2|
|   null|2016|            2|
|   null|2016|            0|
|   null|null|            1|
|   null|null|            3|
+-------+----+-------------+

scala> spark.catalog.listFunctions.filter(_.name.contains("grouping_id")).show(false)
+-----------+--------+-----------+----------------------------------------------------+-----------+
|name       |database|description|className                                           |isTemporary|
+-----------+--------+-----------+----------------------------------------------------+-----------+
|grouping_id|null    |null       |org.apache.spark.sql.catalyst.expressions.GroupingID|true       |
+-----------+--------+-----------+----------------------------------------------------+-----------+

// bin function gives the string representation of the binary value of the given long column
scala> query.withColumn("bitmask", bin($"grouping_id()")).show
+-------+----+-------------+-------+
|   city|year|grouping_id()|bitmask|
+-------+----+-------------+-------+
| Warsaw|2016|            0|      0|
| Warsaw|null|            1|      1|
|Toronto|2017|            0|      0|
|Toronto|2016|            0|      0|
|Toronto|null|            1|      1|
|   null|2017|            2|     10|
|   null|2016|            2|     10|
|   null|2016|            0|      0|  <-- null is city
|   null|null|            3|     11|
|   null|null|            1|      1|
+-------+----+-------------+-------+
----

The list of columns of `grouping_id` should match grouping columns (in `cube` or `rollup`) exactly, or empty which means all the grouping columns (which is exactly what the function expects).

NOTE: `grouping_id` can only be used with link:spark-sql-multi-dimensional-aggregation.adoc#cube[cube], link:spark-sql-multi-dimensional-aggregation.adoc#rollup[rollup] or `GROUPING SETS` multi-dimensional aggregate operators (and is verified when link:spark-sql-Analyzer-CheckAnalysis.adoc#GroupingID[`Analyzer` does check analysis]).

NOTE: Spark SQL's `grouping_id` function is known as `grouping__id` in Hive.

From https://cwiki.apache.org/confluence/display/Hive/Enhanced&#43;Aggregation%2C&#43;Cube%2C&#43;Grouping&#43;and&#43;Rollup#EnhancedAggregation,Cube,GroupingandRollup-Grouping\_\_IDfunction[Hive's documentation about Grouping__ID function]:

> When aggregates are displayed for a column its value is `null`. This may conflict in case the column itself has some `null` values. There needs to be some way to identify `NULL` in column, which means aggregate and `NULL` in column, which means value. `GROUPING__ID` function is the solution to that.

Internally, `grouping_id()` creates a link:spark-sql-Column.adoc[Column] with `GroupingID` unevaluable expression.

NOTE: link:spark-sql-Expression.adoc#Unevaluable[Unevaluable expressions] are expressions replaced by some other expressions during link:spark-sql-Analyzer.adoc[analysis] or link:spark-sql-Optimizer.adoc[optimization].

```
// workshops dataset was defined earlier
val q = workshops
  .cube("city", "year")
  .agg(grouping_id())

// grouping_id function is spark_grouping_id virtual column internally
// that is resolved during analysis - see Analyzed Logical Plan
scala> q.explain(true)
== Parsed Logical Plan ==
'Aggregate [cube(city#182, year#183)], [city#182, year#183, grouping_id() AS grouping_id()#742]
+- Union
   :- Project [_1#178 AS city#182, _2#179 AS year#183, _3#180 AS count#184]
   :  +- LocalRelation [_1#178, _2#179, _3#180]
   +- Project [_1#192 AS city#196, _2#193 AS year#197, _3#194 AS count#198]
      +- LocalRelation [_1#192, _2#193, _3#194]

== Analyzed Logical Plan ==
city: string, year: int, grouping_id(): int
Aggregate [city#757, year#758, spark_grouping_id#754], [city#757, year#758, spark_grouping_id#754 AS grouping_id()#742]
+- Expand [List(city#182, year#183, count#184, city#755, year#756, 0), List(city#182, year#183, count#184, city#755, null, 1), List(city#182, year#183, count#184, null, year#756, 2), List(city#182, year#183, count#184, null, null, 3)], [city#182, year#183, count#184, city#757, year#758, spark_grouping_id#754]
   +- Project [city#182, year#183, count#184, city#182 AS city#755, year#183 AS year#756]
      +- Union
         :- Project [_1#178 AS city#182, _2#179 AS year#183, _3#180 AS count#184]
         :  +- LocalRelation [_1#178, _2#179, _3#180]
         +- Project [_1#192 AS city#196, _2#193 AS year#197, _3#194 AS count#198]
            +- LocalRelation [_1#192, _2#193, _3#194]

== Optimized Logical Plan ==
Aggregate [city#757, year#758, spark_grouping_id#754], [city#757, year#758, spark_grouping_id#754 AS grouping_id()#742]
+- Expand [List(city#755, year#756, 0), List(city#755, null, 1), List(null, year#756, 2), List(null, null, 3)], [city#757, year#758, spark_grouping_id#754]
   +- Union
      :- LocalRelation [city#755, year#756]
      +- LocalRelation [city#755, year#756]

== Physical Plan ==
*HashAggregate(keys=[city#757, year#758, spark_grouping_id#754], functions=[], output=[city#757, year#758, grouping_id()#742])
+- Exchange hashpartitioning(city#757, year#758, spark_grouping_id#754, 200)
   +- *HashAggregate(keys=[city#757, year#758, spark_grouping_id#754], functions=[], output=[city#757, year#758, spark_grouping_id#754])
      +- *Expand [List(city#755, year#756, 0), List(city#755, null, 1), List(null, year#756, 2), List(null, null, 3)], [city#757, year#758, spark_grouping_id#754]
         +- Union
            :- LocalTableScan [city#755, year#756]
            +- LocalTableScan [city#755, year#756]
```

NOTE: `grouping_id` was added to Spark SQL in https://issues.apache.org/jira/browse/SPARK-12706[[SPARK-12706\] support grouping/grouping_id function together group set].

=== [[bin]] Converting Long to Binary Format (in String Representation) -- `bin` Function

[source, scala]
----
bin(e: Column): Column
bin(columnName: String): Column // <1>
----
<1> Calls the first `bin` with `columnName` as a `Column`

`bin` converts the long value in a column to its binary format (i.e. as an unsigned integer in base 2) with no extra leading 0s.

[source, scala]
----
scala> spark.range(5).withColumn("binary", bin('id)).show
+---+------+
| id|binary|
+---+------+
|  0|     0|
|  1|     1|
|  2|    10|
|  3|    11|
|  4|   100|
+---+------+

val withBin = spark.range(5).withColumn("binary", bin('id))
scala> withBin.printSchema
root
 |-- id: long (nullable = false)
 |-- binary: string (nullable = false)
----

Internally, `bin` creates a link:spark-sql-Column.adoc[Column] with `Bin` unary expression.

[source, scala]
----
scala> withBin.queryExecution.logical
res2: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
'Project [*, bin('id) AS binary#14]
+- Range (0, 5, step=1, splits=Some(8))
----

NOTE: `Bin` unary expression uses link:++https://docs.oracle.com/javase/8/docs/api/java/lang/Long.html#toBinaryString-long-++[java.lang.Long.toBinaryString] for the conversion.

[NOTE]
====
`Bin` expression supports link:spark-sql-Expression.adoc#doGenCode[code generation] (aka _CodeGen_).

```
val withBin = spark.range(5).withColumn("binary", bin('id))
scala> withBin.queryExecution.debug.codegen
Found 1 WholeStageCodegen subtrees.
== Subtree 1 / 1 ==
*Project [id#19L, bin(id#19L) AS binary#22]
+- *Range (0, 5, step=1, splits=Some(8))
...
/* 103 */           UTF8String project_value1 = null;
/* 104 */           project_value1 = UTF8String.fromString(java.lang.Long.toBinaryString(range_value));

```
====
